{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fac1142f-e5fe-4eb6-87e7-0c651602b460",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "API_KEY = \"your_api_key\"\n",
    "client = OpenAI(api_key=API_KEY, base_url=\"https://api.siliconflow.cn/v1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abb604b6-b235-494e-9077-64bff4ce0691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: (120, 4)\n",
      "Test set size: (30, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# Features and target\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Training set size:\", X_train.shape)\n",
    "print(\"Test set size:\", X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40319d69-d705-4790-935f-f0e6f5c41da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Accuracy: 0.9\n"
     ]
    }
   ],
   "source": [
    "## Use the classical ML model of random forest for classification\n",
    "\n",
    "# Initialize and train the model\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"\\nRandom Forest Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c71f646-84a7-441f-89ba-76609c507168",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Test Example 0\n",
      "Predicting Test Example 1\n",
      "Predicting Test Example 2\n",
      "Predicting Test Example 3\n",
      "Predicting Test Example 4\n",
      "Predicting Test Example 5\n",
      "Predicting Test Example 6\n",
      "Predicting Test Example 7\n",
      "Predicting Test Example 8\n",
      "Predicting Test Example 9\n",
      "Predicting Test Example 10\n",
      "Predicting Test Example 11\n",
      "Predicting Test Example 12\n",
      "Predicting Test Example 13\n",
      "Predicting Test Example 14\n",
      "Predicting Test Example 15\n",
      "Predicting Test Example 16\n",
      "Predicting Test Example 17\n",
      "Predicting Test Example 18\n",
      "Predicting Test Example 19\n",
      "Predicting Test Example 20\n",
      "Predicting Test Example 21\n",
      "Predicting Test Example 22\n",
      "Predicting Test Example 23\n",
      "Predicting Test Example 24\n",
      "Predicting Test Example 25\n",
      "Predicting Test Example 26\n",
      "Predicting Test Example 27\n",
      "Predicting Test Example 28\n",
      "Predicting Test Example 29\n"
     ]
    }
   ],
   "source": [
    "\n",
    "N = len(X_test)\n",
    "true_label = []\n",
    "pred_label = []\n",
    "\n",
    "for n in range(N):\n",
    "    print(\"Predicting Test Example\", n)\n",
    "\n",
    "    \n",
    "    # Here we construct the prompt for querying the LLM\n",
    "    prompt = \"Help me predict the Output value for the last Input. Your response should only contain the Output value in the format of #Output value#.\\n\"\n",
    "\n",
    "    s = \"\"\n",
    "    for i in np.arange(len(X_train)):\n",
    "        s += f\"Input: {X_train[i]}, Output: {y_train[i]}\\n\"\n",
    "    s += f\"Input: \" + str(X_test[n]) + \", Output: \"\n",
    "\n",
    "    prompt += s\n",
    "    # print(prompt)\n",
    "\n",
    "    \n",
    "    # Sometimes the LLM may not return our desired results. So, we try we try querying the LLM up to max_tries times. If still unsuccessful, we return a random label as prediction.\n",
    "    max_tries = 5\n",
    "    err_counter = 0\n",
    "    while err_counter < max_tries:\n",
    "        try:\n",
    "            completion = client.chat.completions.create(\n",
    "                # model='Qwen/Qwen2-1.5B-Instruct',\n",
    "                model='Qwen/Qwen2-7B-Instruct',\n",
    "                messages=[\n",
    "                    {'role': 'user', \n",
    "                    'content': prompt}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            response = completion.choices[0].message.content\n",
    "            pred = int(response.replace(\"#\", \"\"))\n",
    "\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Error encountered: {e}. Retrying...\")\n",
    "            err_counter += 1\n",
    "\n",
    "    if err_counter == max_tries:\n",
    "        # if still unsuccessful after \"max_tries\" tries, return a random label\n",
    "        print(\"max number of tries exceeded\")\n",
    "        pred = random.randint(0, 2)\n",
    "\n",
    "    true_label.append(y_test[n])\n",
    "    pred_label.append(pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e3b148e-6824-4a52-9a56-24e8f9fef3be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groundtrugh labels:\n",
      "[0, 2, 1, 1, 0, 1, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 2, 0, 2, 1, 2, 2, 1, 1, 0, 2, 0]\n",
      "Predicted labels by ICL:\n",
      "[0, 2, 0, 1, 0, 2, 0, 0, 2, 1, 2, 2, 2, 1, 0, 0, 0, 1, 1, 1, 0, 2, 1, 2, 2, 2, 1, 0, 2, 0]\n",
      "\n",
      "ICL Accuracy: 0.8666666666666667\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Groundtrugh labels:\")\n",
    "print(true_label)\n",
    "print(\"Predicted labels by ICL:\")\n",
    "print(pred_label)\n",
    "\n",
    "accuracy = accuracy_score(true_label, pred_label)\n",
    "print(\"\\nICL Accuracy:\", accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
